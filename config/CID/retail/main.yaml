# dataset config : Sequential Recommendation
data_path: ./data/processed

format: sequential
target: True
behavior: True
item_augmented: False
seq_augmented: False

#tokenizer
tokenizer_type: Chunked
K: 64
item_len: 3
max_seq_len: 202
min_seq_len: 1
behavior_tokens: [1,2,3,4]
id_offsets: [0, 5, 69, 133, 197] # token offset for each layer of semantic token, 1-4(behaviors)(since behavior IDs start from 1, we set the offset to be 0), 5-68, 69-132, 133-196.
BOS_token: 0 # BOS must be set to 0, EOS is automatically set to be the last token after BOS, special, and item tokens
num_user_tokens: 2000
random_map: True
# TIGER:

# T5:
T5_type: "PBA"
behavior_injection: True
behavior_embedding_dim: 64
num_positions: 4
num_behavior: 4
num_layers: 4
num_decoder_layers: 4
num_sparse_encoder_layers: 4
num_sparse_decoder_layers: 4
sparse_layers_encoder: []
sparse_layers_decoder: [0, 1, 2, 3]
behavior_injection_encoder: [0, 1]
behavior_injection_decoder: [0, 1]
d_model: 256
d_ff: 512
num_heads: 6
d_kv: 64
dropout_rate: 0.0
activation_function: "relu"
feed_forward_proj: "relu"
n_positions: 50

#trainer:
exp_id: "CID_retail"
steps: 350000
lr: 0.001
warmup_steps: 15000
weight_decay: 0.005
batch_size: 512
eval_batch_size: 32
patience: 10
auto_save_epochs: 5
epochs_per_eval: 1
epochs_per_all_eval: 1
decoder_input: [0,1] # 0: BOS, 1: target behavior token